{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_items = 1000\n",
    "x = torch.rand(num_items, 2)\n",
    "c = x.round()\n",
    "y = (c.sum(dim=1) == 1).float()[:, None]\n",
    "x_train, x_test, c_train, c_test, y_train, y_test = train_test_split(x, c, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_features, n_concepts, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_concepts = n_concepts\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.concept_context_generator = torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_features, 2 * emb_size * n_concepts),\n",
    "                torch.nn.LeakyReLU(),\n",
    "            )\n",
    "        self.concept_prob_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * emb_size, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        concept_embs = self.concept_context_generator(x)\n",
    "        concept_embs_shape = x.shape[:-1] + (self.n_concepts, 2 * self.emb_size)\n",
    "        concept_embs = concept_embs.view(*concept_embs_shape)\n",
    "        concept_probs = self.concept_prob_predictor(concept_embs)\n",
    "        concept_pos = concept_embs[..., :self.emb_size]\n",
    "        concept_neg = concept_embs[..., self.emb_size:]\n",
    "        concept_embs = concept_pos * concept_probs + concept_neg * (1 - concept_probs)\n",
    "        return concept_embs, concept_probs.squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8\n",
    "concept_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x.shape[1], 10),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    ConceptEmbedding(10, c.shape[1], embedding_size),\n",
    ")\n",
    "task_predictor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(c.shape[1]*embedding_size, 1),\n",
    ")\n",
    "model = torch.nn.Sequential(concept_encoder, task_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_form_c = torch.nn.BCELoss()\n",
    "loss_form_y = torch.nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epoch in range(501):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # generate concept and task predictions\n",
    "    c_emb, c_pred = concept_encoder(x_train)\n",
    "    y_pred = task_predictor(c_emb.reshape(len(c_emb), -1))\n",
    "    # compute loss\n",
    "    concept_loss = loss_form_c(c_pred, c_train)\n",
    "    task_loss = loss_form_y(y_pred, y_train)\n",
    "    loss = concept_loss + 0.5*task_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 0.98, Concept accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "c_emb, c_pred = concept_encoder.forward(x_test)\n",
    "y_pred = task_predictor(c_emb.reshape(len(c_emb), -1))\n",
    "\n",
    "task_accuracy = accuracy_score(y_test, y_pred > 0)\n",
    "concept_accuracy = accuracy_score(c_test, c_pred > 0.5)\n",
    "print(f\"Task accuracy: {task_accuracy:.2f}, Concept accuracy: {concept_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = F.one_hot(y_train.long().ravel()).float()\n",
    "y_test = F.one_hot(y_test.long().ravel()).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptMemoryReasoningLayer(torch.nn.Module):\n",
    "    def __init__(self, n_concepts, n_classes, emb_size=32, n_rules=2):\n",
    "        super(ConceptMemoryReasoningLayer, self).__init__()\n",
    "        self.n_concepts = n_concepts\n",
    "        self.n_classes = n_classes\n",
    "        self.n_rules = n_rules\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.rule_book = nn.Embedding(n_classes * n_rules, self.emb_size)\n",
    "\n",
    "        self.rule_decoder = nn.Sequential(\n",
    "            nn.Linear(self.emb_size, self.emb_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.emb_size, 3 * n_concepts),\n",
    "        )\n",
    "\n",
    "        \"\"\"self.rule_decoder_highend = nn.Embedding(n_classes * n_rules * n_concepts, 3)\"\"\"\n",
    "\n",
    "        self.rule_selector = nn.Sequential(\n",
    "            nn.Linear(self.n_concepts, self.emb_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.emb_size, n_classes * n_rules),\n",
    "        )\n",
    "    \n",
    "    def decode_rules(self):\n",
    "        rule_embs = self.rule_book.weight.view(self.n_classes, self.n_rules, self.emb_size)\n",
    "        rules_decoded = self.rule_decoder(rule_embs).view(self.n_classes, self.n_rules, self.n_concepts, 3)\n",
    "        #rules_decoded = self.rule_decoder_highend.weight.view(self.n_classes, self.n_rules, self.n_concepts, 3)\n",
    "        rules_decoded = F.softmax(rules_decoded, dim=-1)\n",
    "        if not self.training:\n",
    "            # argmax to get the most likely rule\n",
    "            rules_decoded = F.one_hot(torch.argmax(rules_decoded, dim=-1), num_classes=3)\n",
    "        return rules_decoded\n",
    "\n",
    "\n",
    "    def forward(self, x, return_explanation=False, concept_names=None):\n",
    "        \n",
    "        rules_decoded = self.decode_rules()\n",
    "        rule_scores = self.rule_selector(x).view(-1, self.n_classes, self.n_rules)\n",
    "        rule_scores = F.softmax(rule_scores, dim=-1) # (batch_dim, n_classes, n_rules)\n",
    "        if not self.training:\n",
    "            # argmax to get the most likely rule\n",
    "            rule_scores = F.one_hot(torch.argmax(rule_scores, dim=-1), num_classes=self.n_rules)\n",
    "        \n",
    "        agg_rules = (rules_decoded[None, ...] * rule_scores[..., None, None]) # # (batch_dim, n_classes, n_rules, n_concepts, 3)\n",
    "        agg_rules = agg_rules.sum(dim=-3) \n",
    "        pos_rules = agg_rules[..., 0] \n",
    "        neg_rules = agg_rules[..., 1] \n",
    "        irr_rules = agg_rules[..., 2]\n",
    "        x = x[..., None, :]\n",
    "        # batch_dim, n_classes, n_concepts\n",
    "        preds = (pos_rules * x + neg_rules * (1 - x) + irr_rules).prod(dim=-1)\n",
    "        c_rec = 0.5 * irr_rules + pos_rules\n",
    "        \n",
    "        \n",
    "        aux_loss = F.binary_cross_entropy(c_rec, x.repeat(1, c_rec.shape[1], 1),reduction=\"none\").mean(dim=-1)\n",
    "        aux_loss = (aux_loss * preds).mean()\n",
    "\n",
    "        explanations = None\n",
    "        assert not return_explanation or not self.training, \"Explanation can only be returned in eval mode\"\n",
    "        if return_explanation:\n",
    "            if concept_names is None:\n",
    "                concept_names = [f\"c_{i}\" for i in range(self.n_concepts)]\n",
    "            rule_counts = (rule_scores.round().to(torch.long) * preds[..., None].round().to(torch.long)).sum(dim=0) # (n_classes, n_rules)\n",
    "            from collections import defaultdict\n",
    "            rule_strings = defaultdict(list)\n",
    "            explanations = {}\n",
    "            for i in range(self.n_classes):\n",
    "                class_name = f\"y_{i}\"\n",
    "                for j in range(self.n_rules):\n",
    "                    for c in range(self.n_concepts):\n",
    "                        is_pos = rules_decoded[i, j, c, 0]\n",
    "                        is_neg = rules_decoded[i, j, c, 1]\n",
    "                        is_irr = rules_decoded[i, j, c, 2]\n",
    "                        if is_pos > 0.5:\n",
    "                            rule_strings[(i, j)].append(concept_names[c])\n",
    "                        elif is_neg > 0.5:\n",
    "                            rule_strings[(i, j)].append(f\"~{concept_names[c]}\")\n",
    "                explanations[class_name] = []\n",
    "                for j in range(self.n_rules):\n",
    "                    if rule_counts[i, j] > 0:\n",
    "                        rule_str = \" & \".join(rule_strings[(i, j)])\n",
    "                        explanations[class_name].append(f\"Rule {j}: {rule_str} (Counts: {rule_counts[i, j]})\")\n",
    "\n",
    "        return preds, aux_loss, explanations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_predictor = ConceptMemoryReasoningLayer(2, y_train.shape[1], emb_size=embedding_size)\n",
    "model = torch.nn.Sequential(concept_encoder, task_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_emb, c_pred = concept_encoder(x_train)\n",
    "y_pred, aux_loss, _ = task_predictor(c_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([670, 2]), torch.Size([]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape, aux_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([670, 2, 8]), torch.Size([670, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_encoder(x_train)[0].shape, concept_encoder(x_train)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([670, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_form = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(512):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # generate concept and task predictions\n",
    "        c_emb, c_pred = concept_encoder(x_train)\n",
    "        y_pred, aux_loss, _ = task_predictor(c_pred)\n",
    "\n",
    "        #aux_loss = (y_train * aux_loss).mean() \n",
    "        # compute loss\n",
    "        concept_loss = loss_form(c_pred, c_train)\n",
    "        task_loss = loss_form(y_pred, y_train)\n",
    "        loss = concept_loss + 0.5*task_loss + 2 * aux_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 0.],\n",
       "          [0., 1., 0.]],\n",
       "\n",
       "         [[1., 0., 0.],\n",
       "          [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0.],\n",
       "          [1., 0., 0.]],\n",
       "\n",
       "         [[1., 0., 0.],\n",
       "          [0., 1., 0.]]]], grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_predictor.decode_rules().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_predictor.eval()\n",
    "\n",
    "x_test = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y_test = F.one_hot(torch.tensor([0, 1, 1, 0]), num_classes=2)\n",
    "task_predictor(x_test)[0].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y_0': ['Rule 0: ~c_0 & ~c_1 (Counts: 1)', 'Rule 1: c_0 & c_1 (Counts: 1)'],\n",
       " 'y_1': ['Rule 0: ~c_0 & c_1 (Counts: 1)', 'Rule 1: c_0 & ~c_1 (Counts: 1)']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_predictor(x_test, return_explanation=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task accuracy: 1.00 Auxillary loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "c_emb, c_pred = concept_encoder.forward(x_test)\n",
    "y_pred_, aux_loss_, explanations = task_predictor(c_pred)\n",
    "\n",
    "aux_loss_ = (y_test * aux_loss_).mean()\n",
    "task_accuracy = accuracy_score(y_test, y_pred_ > 0.5)\n",
    "#concept_accuracy = accuracy_score(c_test, c_pred > 0.5)\n",
    "print(f\"Task accuracy: {task_accuracy:.2f}\", f\"Auxillary loss: {aux_loss_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_explanations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlocal_explanations\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_explanations' is not defined"
     ]
    }
   ],
   "source": [
    "local_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
